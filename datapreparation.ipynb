{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and become familiar with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install germanetpy==0.2.2\n",
    "from pathlib import Path\n",
    "from germanetpy import germanet\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import spacy\n",
    "import swifter\n",
    "import re\n",
    "import nltk\n",
    "import codecs\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.stem.cistem import Cistem\n",
    "from collections import Counter\n",
    "from spacy.lang.de.examples import sentences \n",
    "#import for graphs:\n",
    "#!pip3 install bokeh \n",
    "from bokeh.plotting import ColumnDataSource\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import show\n",
    "from bokeh.plotting import output_file\n",
    "from bokeh.transform import factor_cmap\n",
    "#!pip install matplotlib\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as pl\n",
    "#loading de_core_news_sm-3.0.0 for preprocessing\n",
    "nlp = spacy.load(\"/Users/anna/Desktop/NLP/sciebo-code_PowerPuff/de_core_news_sm-3.0.0/de_core_news_sm/de_core_news_sm-3.0.0\")\n",
    "#germanet import\n",
    "germanet_object = germanet.Germanet(\"/Users/anna/germanet_data/GN_V170_XML\")\n",
    "#loading Datset\n",
    "df = pd.read_csv(r'/Users/anna/Desktop/NLP/sciebo-code_PowerPuff/suggestions_minorities_slice.csv', delimiter=\",\", error_bad_lines=False, engine ='python')\n",
    "df.head(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "class GSBertPolarityModel:\n",
    "    \n",
    "    def __init__(self, model_name: str = \"oliverguhr/german-sentiment-bert\"):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        # Always use original tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"oliverguhr/german-sentiment-bert\")\n",
    "\n",
    "        self.clean_chars = re.compile(r'[^A-Za-züöäÖÜÄß ]', re.MULTILINE)\n",
    "        self.clean_http_urls = re.compile(r'https*\\S+', re.MULTILINE)\n",
    "        self.clean_at_mentions = re.compile(r'@\\S+', re.MULTILINE)\n",
    "\n",
    "    def replace_numbers(self, text: str) -> str:\n",
    "        return text.replace(\"0\", \" null\").replace(\"1\", \" eins\").replace(\"2\", \" zwei\") \\\n",
    "            .replace(\"3\", \" drei\").replace(\"4\", \" vier\").replace(\"5\", \" fünf\") \\\n",
    "            .replace(\"6\", \" sechs\").replace(\"7\", \" sieben\").replace(\"8\", \" acht\") \\\n",
    "            .replace(\"9\", \" neun\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = self.clean_http_urls.sub('', text)\n",
    "        text = self.clean_at_mentions.sub('', text)\n",
    "        text = self.replace_numbers(text)\n",
    "        text = self.clean_chars.sub('', text)  # use only text chars\n",
    "        text = ' '.join(text.split())  # substitute multiple whitespace with single whitespace\n",
    "        text = text.strip().lower()\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def probs2polarities(pnn: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Transform softmax probs of a [positive, negative, neutral] classifier\n",
    "        into scalar polarity scores of range [-1, 1].\n",
    "        High values express positive sentiment, low negative ones negative sentiment.\n",
    "        Values close to 0 express neutral sentiment.\"\"\"\n",
    "        pos = pnn[:, 0]\n",
    "        neg = pnn[:, 1]\n",
    "        # Transform range [0, 1] to [-1, 1]\n",
    "        # Ignore neutrality score as it's implicitly encoded as (1 - pos - neg)\n",
    "        polarities = pos - neg\n",
    "        return polarities\n",
    "\n",
    "    def predict_sentiment_batch(self, texts: List[str]) -> torch.Tensor:\n",
    "        texts = [self.clean_text(text) for text in texts]\n",
    "        # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "        input_ids = self.tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True  # Ensure that the text does not exceed the token limit\n",
    "        )\n",
    "        input_ids = torch.tensor(input_ids[\"input_ids\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids)\n",
    "            probs = F.softmax(logits[0], dim=1)\n",
    "\n",
    "        polarities = self.probs2polarities(probs)\n",
    "        return polarities\n",
    "\n",
    "    def analyse_sentiment(self, text: str) -> float:\n",
    "        polarity = self.predict_sentiment_batch([text]).item()\n",
    "        return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#looking for unique terms in queryterm for seeing the different categories\n",
    "unique_queryterm=np.unique(df['queryterm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data cleaning: remove distracting characters \n",
    "df['cleaned'] = df['raw_data'].replace(regex = ['{\"q\":.+?.}|\\\\\\\\u003c\\\\\\\\.b|,\"t\":{\"bpc\":false,\"tlw\":false}|,0|\"phrase\":|\\\\\\\\u003e|\\\\\\\\u003cb|\\\\\\\\u003cVb|\\\\\\\\u003cse|\\\\\\\\u003csc'], value='') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#removing _minorities\n",
    "df['label'] = df['label'].replace(regex = [r'_minorities'], value='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#tokenization of cleaned and queryterm and create a new dataframe\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens_suggestions\"] = df.apply(lambda row: tokenizer.tokenize(str(row[\"cleaned\"].lower())), axis=1)\n",
    "df[\"tokensroot\"] = df.apply(lambda row: tokenizer.tokenize(str(row[\"queryterm\"].lower())), axis=1)\n",
    "#df[\"spacy_tokens\"] = df.apply(lambda row: nlp(str(row[\"tokensroot\"])), axis=1)\n",
    "tokensroot2 = df['tokensroot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#using nlp ergo spacy for tokensroot\n",
    "tokensroot = nlp(str(df['tokensroot']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Is the queryterm a question or not?; 0=no question, 1=question\n",
    "n = 200000\n",
    "i = 0\n",
    "stemmer = Cistem()\n",
    "german_stop_words = stopwords.words('german')\n",
    "df['Frage?'] = 0\n",
    "df['tokens_suggestions_cleaned_root'] = ''\n",
    "df['stemmed_words'] = ''\n",
    "df['synsets_ger'] = ''\n",
    "questionTerms = ['ist', 'sind', 'sollen', 'wann', 'warum', 'wenn', 'weshalb', 'wo', 'wollen']\n",
    "cleaned_token_suggestions = []\n",
    "for row in df.itertuples(index=True, name='Pandas'):\n",
    "    synset = []\n",
    "    cleanedSuggestions = list(set(row.tokens_suggestions))\n",
    "    df.at[row.Index, 'tokens_suggestions_cleaned_root'] = cleanedSuggestions\n",
    "    if row.tokensroot[0] in questionTerms:\n",
    "        df.at[row.Index, 'Frage?'] = 1\n",
    "    for tokensroot in row.tokensroot:\n",
    "        if tokensroot in row.tokens_suggestions:\n",
    "            cleanedSuggestions.pop(cleanedSuggestions.index(tokensroot))\n",
    "            df.at[row.Index, 'tokens_suggestions_cleaned_root'] = cleanedSuggestions\n",
    "            for cleanedSuggestion in cleanedSuggestions:\n",
    "                synsetToken = germanet_object.get_synsets_by_orthform(cleanedSuggestion)\n",
    "                if synsetToken != []:\n",
    "                    synset.append(synsetToken)\n",
    "        if tokensroot not in german_stop_words and tokensroot not in questionTerms:\n",
    "            stemmedTokensroot = stemmer.stem(tokensroot)\n",
    "            df.at[row.Index, 'stemmed_words'] = stemmedTokensroot\n",
    "    df.at[row.Index, 'synsets_ger'] = synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting hypernyms and hyponyms of synsets_ger to dataframe lexunits\n",
    "def get_custom_names_synsets(syns):\n",
    "    ret = []\n",
    "    try:\n",
    "        for cleanedSyns in syns:\n",
    "            for syn in cleanedSyns:\n",
    "                for lemma in syn.lexunits:\n",
    "                    ret.extend(lemma.get_all_orthforms())\n",
    "    #print(ret)\n",
    "        return ret\n",
    "    except:\n",
    "        return ret\n",
    "df[\"lexunits\"] = df.apply(lambda row: get_custom_names_synsets(row[\"synsets_ger\"]), axis=1)\n",
    "\n",
    "    \n",
    "def get_hypernyms_from_list_of_synsets(list_syns):\n",
    "    directHypernyms = []\n",
    "    for second_level_list_syns in list_syns:\n",
    "        for syn in second_level_list_syns:\n",
    "            directHypernyms.append(syn.direct_hypernyms)\n",
    "    return directHypernyms\n",
    "df[\"hypernyms\"] = df.apply(lambda row: get_hypernyms_from_list_of_synsets(row[\"synsets_ger\"]), axis=1)\n",
    "df[\"lexunits_hypernyms\"] = df.apply(lambda row: get_custom_names_synsets(row[\"hypernyms\"]), axis=1)\n",
    "\n",
    "def get_hyponyms_from_list_of_synsets(list_syns):\n",
    "    directHyponyms = []\n",
    "    for second_level_list_syns in list_syns:\n",
    "        for syn in second_level_list_syns:\n",
    "            directHyponyms.append(syn.direct_hyponyms)\n",
    "    return directHyponyms\n",
    "df[\"hyponyms\"] = df.apply(lambda row: get_hyponyms_from_list_of_synsets(row[\"synsets_ger\"]), axis=1)\n",
    "df[\"lexunits_hyponyms\"] = df.apply(lambda row: get_custom_names_synsets(row[\"hyponyms\"]), axis=1)\n",
    "#df[[\"suggestion_ger\",\"lexunits_hypernyms\", \"lexunits_hyponyms\", \"lexunits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee48a36c858e4296806a6b2b516a2124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/52464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2e6fecd9ff41139040160c9cb4b892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/52464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tensor results for tokens_suggestions_cleaned_root and the lexunits tensors of hypernyms\n",
    "model = GSBertPolarityModel()\n",
    "df[\"tokens_suggestions_tensor\"] = ''\n",
    "df[\"lexunits_tensor\"] = ''\n",
    "def get_tensor(words):\n",
    "    if words != []:\n",
    "        word = list(dict.fromkeys(words))\n",
    "        filter(lambda x: x in german_stop_words, words)\n",
    "        return model.predict_sentiment_batch(words)\n",
    "    else:\n",
    "        return ''\n",
    "df[\"lexunits_tensor\"] = df.lexunits.swifter.apply(lambda row: get_tensor(row))\n",
    "df[\"tokens_suggestions_tensor\"] = df.tokens_suggestions_cleaned_root.swifter.apply(lambda row: get_tensor(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e368d24c5e86432e96e7caf52e118726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/52464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf557f9ab9a4d8791cef8aedf98bf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/52464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suggestion_id</th>\n",
       "      <th>label</th>\n",
       "      <th>queryterm</th>\n",
       "      <th>date</th>\n",
       "      <th>client</th>\n",
       "      <th>lang</th>\n",
       "      <th>geolocation</th>\n",
       "      <th>url</th>\n",
       "      <th>proxy_ip</th>\n",
       "      <th>proxy_city</th>\n",
       "      <th>...</th>\n",
       "      <th>synsets_ger</th>\n",
       "      <th>lexunits</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>lexunits_hypernyms</th>\n",
       "      <th>hyponyms</th>\n",
       "      <th>lexunits_hyponyms</th>\n",
       "      <th>tokens_suggestions_tensor</th>\n",
       "      <th>lexunits_tensor</th>\n",
       "      <th>tokens_suggestions_tensor_sum</th>\n",
       "      <th>lexunits_tensor_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4538056</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>türke</td>\n",
       "      <td>2020-05-03 08:05:01.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.8585), tensor(0.9375), tensor(-0.01...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(1.6650)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4538057</td>\n",
       "      <td>google_firefox</td>\n",
       "      <td>türke</td>\n",
       "      <td>2020-05-03 08:05:01.0</td>\n",
       "      <td>firefox</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.8585), tensor(-0.0100), tensor(-0.8...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(0.5884)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4538058</td>\n",
       "      <td>ddg</td>\n",
       "      <td>türke</td>\n",
       "      <td>2020-05-03 08:05:01.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.8837), tensor(-0.6965), tensor(0.63...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-1.3746)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4538059</td>\n",
       "      <td>bing</td>\n",
       "      <td>türke</td>\n",
       "      <td>2020-05-03 08:05:01.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s55185, lexunits=zurücktreten), Sy...</td>\n",
       "      <td>[zurücktreten, zurücktreten, zurücktreten, zur...</td>\n",
       "      <td>[{Synset(id=s55172, lexunits=aufgeben, verzich...</td>\n",
       "      <td>[aufgeben, verzichten, versagen, ausscheiden, ...</td>\n",
       "      <td>[{}, {Synset(id=s80699, lexunits=demissioniere...</td>\n",
       "      <td>[demissionieren, beitragsfrei, gebührenfrei, g...</td>\n",
       "      <td>[tensor(-0.8585), tensor(0.9375), tensor(0.433...</td>\n",
       "      <td>[tensor(-0.9261), tensor(-0.9261), tensor(-0.9...</td>\n",
       "      <td>tensor(3.1092)</td>\n",
       "      <td>tensor(-2.4034)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4538060</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>pole</td>\n",
       "      <td>2020-05-03 08:05:02.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s66916, lexunits=polen)], [Synset(...</td>\n",
       "      <td>[polen, polemisch]</td>\n",
       "      <td>[{Synset(id=s56823, lexunits=anschließen)}, {S...</td>\n",
       "      <td>[anschließen, stilistisch, provozierend, provo...</td>\n",
       "      <td>[{Synset(id=s80471, lexunits=umpolen)}, {}]</td>\n",
       "      <td>[umpolen]</td>\n",
       "      <td>[tensor(-0.7592), tensor(-0.8762), tensor(0.81...</td>\n",
       "      <td>[tensor(-0.8216), tensor(-0.8856)]</td>\n",
       "      <td>tensor(-2.8080)</td>\n",
       "      <td>tensor(-1.7072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4538061</td>\n",
       "      <td>google_firefox</td>\n",
       "      <td>pole</td>\n",
       "      <td>2020-05-03 08:05:02.0</td>\n",
       "      <td>firefox</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s66916, lexunits=polen)], [Synset(...</td>\n",
       "      <td>[polen, polemisch]</td>\n",
       "      <td>[{Synset(id=s56823, lexunits=anschließen)}, {S...</td>\n",
       "      <td>[anschließen, stilistisch, provozierend, provo...</td>\n",
       "      <td>[{Synset(id=s80471, lexunits=umpolen)}, {}]</td>\n",
       "      <td>[umpolen]</td>\n",
       "      <td>[tensor(-0.7592), tensor(-0.8762), tensor(0.81...</td>\n",
       "      <td>[tensor(-0.8216), tensor(-0.8856)]</td>\n",
       "      <td>tensor(-3.7279)</td>\n",
       "      <td>tensor(-1.7072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4538062</td>\n",
       "      <td>bing</td>\n",
       "      <td>pole</td>\n",
       "      <td>2020-05-03 08:05:03.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s66916, lexunits=polen)], [Synset(...</td>\n",
       "      <td>[polen, italienisch]</td>\n",
       "      <td>[{Synset(id=s56823, lexunits=anschließen)}, {S...</td>\n",
       "      <td>[anschließen, südeuropäisch, südländisch]</td>\n",
       "      <td>[{Synset(id=s80471, lexunits=umpolen)}, {Synse...</td>\n",
       "      <td>[umpolen, mittelitalienisch, süditalienisch, n...</td>\n",
       "      <td>[tensor(0.1253), tensor(-0.8762), tensor(0.940...</td>\n",
       "      <td>[tensor(-0.8196), tensor(0.6341)]</td>\n",
       "      <td>tensor(-0.6651)</td>\n",
       "      <td>tensor(-0.1856)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4538063</td>\n",
       "      <td>ddg</td>\n",
       "      <td>pole</td>\n",
       "      <td>2020-05-03 08:05:03.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(0.8301), tensor(0.7862), tensor(0.1622...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(3.4194)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4538064</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>rumäne</td>\n",
       "      <td>2020-05-03 08:05:03.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig, aussehen, auss...</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}, {Synset(...</td>\n",
       "      <td>[modern, sein, suchen, sein, fehlinformieren]</td>\n",
       "      <td>[{}, {Synset(id=s52138, lexunits=ausnehmen), S...</td>\n",
       "      <td>[ausnehmen, wirken, schauen, rumlügen, heuchel...</td>\n",
       "      <td>[tensor(0.9491), tensor(0.7781), tensor(-0.949...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-4.1615)</td>\n",
       "      <td>tensor(-3.5504)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4538065</td>\n",
       "      <td>bing</td>\n",
       "      <td>rumäne</td>\n",
       "      <td>2020-05-03 08:05:05.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig, funktionieren,...</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}, {Synset(...</td>\n",
       "      <td>[modern, passieren, ereignen, denken, befassen...</td>\n",
       "      <td>[{}, {Synset(id=s105884, lexunits=nachgehen), ...</td>\n",
       "      <td>[nachgehen, vorgehen, leerlaufen, zuarbeiten, ...</td>\n",
       "      <td>[tensor(0.6576), tensor(0.7320), tensor(-0.841...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-1.1195)</td>\n",
       "      <td>tensor(0.9746)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4538066</td>\n",
       "      <td>google_firefox</td>\n",
       "      <td>rumäne</td>\n",
       "      <td>2020-05-03 08:05:04.0</td>\n",
       "      <td>firefox</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig, aussehen, auss...</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}, {Synset(...</td>\n",
       "      <td>[modern, sein, suchen, sein, fehlinformieren]</td>\n",
       "      <td>[{}, {Synset(id=s52138, lexunits=ausnehmen), S...</td>\n",
       "      <td>[ausnehmen, wirken, schauen, rumlügen, heuchel...</td>\n",
       "      <td>[tensor(0.9491), tensor(0.7781), tensor(-0.949...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-4.1615)</td>\n",
       "      <td>tensor(-3.5504)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4538067</td>\n",
       "      <td>bing</td>\n",
       "      <td>grieche</td>\n",
       "      <td>2020-05-03 08:05:06.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(0.9799), tensor(-0.0652), tensor(0.937...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(0.0468)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4538068</td>\n",
       "      <td>ddg</td>\n",
       "      <td>rumäne</td>\n",
       "      <td>2020-05-03 08:05:05.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.8853), tensor(-0.8547), tensor(0.57...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-4.7053)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4538069</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>grieche</td>\n",
       "      <td>2020-05-03 08:05:05.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.8435), tensor(0.9319), tensor(0.786...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(3.8756)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4538070</td>\n",
       "      <td>bing</td>\n",
       "      <td>kroate</td>\n",
       "      <td>2020-05-03 08:05:07.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig, plündern, abrä...</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}, {Synset(...</td>\n",
       "      <td>[modern, wegnehmen, abnehmen, fortnehmen]</td>\n",
       "      <td>[{}, {Synset(id=s52520, lexunits=marodieren), ...</td>\n",
       "      <td>[marodieren, fleddern]</td>\n",
       "      <td>[tensor(-0.8276), tensor(-0.9004), tensor(0.80...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-6.0244)</td>\n",
       "      <td>tensor(-1.1680)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4538071</td>\n",
       "      <td>google_firefox</td>\n",
       "      <td>grieche</td>\n",
       "      <td>2020-05-03 08:05:07.0</td>\n",
       "      <td>firefox</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.8435), tensor(0.9319), tensor(0.786...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(3.8756)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4538072</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>kroate</td>\n",
       "      <td>2020-05-03 08:05:08.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(0.0378), tensor(-0.9932), tensor(0.836...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-1.3039)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4538073</td>\n",
       "      <td>ddg</td>\n",
       "      <td>grieche</td>\n",
       "      <td>2020-05-03 08:05:07.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(0.7365), tensor(0.7808), tensor(0.7474...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(0.4034)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4538074</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>bulgare</td>\n",
       "      <td>2020-05-03 08:05:09.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig, türkisch]</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}, {Synset(...</td>\n",
       "      <td>[modern, eurasisch]</td>\n",
       "      <td>[{}, {Synset(id=s78994, lexunits=anatolisch), ...</td>\n",
       "      <td>[anatolisch, trojanisch]</td>\n",
       "      <td>[tensor(-0.9799), tensor(-0.9274), tensor(0.68...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-3.8855)</td>\n",
       "      <td>tensor(2.4875)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4538075</td>\n",
       "      <td>bing</td>\n",
       "      <td>bulgare</td>\n",
       "      <td>2020-05-03 08:05:08.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig]</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}]</td>\n",
       "      <td>[modern]</td>\n",
       "      <td>[{}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(0.7969), tensor(0.8885), tensor(-0.979...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-3.7403)</td>\n",
       "      <td>tensor(1.5943)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4538076</td>\n",
       "      <td>ddg</td>\n",
       "      <td>kroate</td>\n",
       "      <td>2020-05-03 08:05:09.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(0.4713), tensor(-0.6715), tensor(0.217...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-2.8406)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4538077</td>\n",
       "      <td>google_firefox</td>\n",
       "      <td>kroate</td>\n",
       "      <td>2020-05-03 08:05:09.0</td>\n",
       "      <td>firefox</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(0.0378), tensor(-0.9932), tensor(0.836...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-1.3039)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4538078</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>afghane</td>\n",
       "      <td>2020-05-03 08:05:10.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig, essen, futtern...</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}, {Synset(...</td>\n",
       "      <td>[modern, verzehren]</td>\n",
       "      <td>[{}, {Synset(id=s149627, lexunits=anfuttern), ...</td>\n",
       "      <td>[anfuttern, verfressen, verfuttern, hinuntersc...</td>\n",
       "      <td>[tensor(0.7558), tensor(-0.9794), tensor(0.611...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(0.7344)</td>\n",
       "      <td>tensor(2.2359)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4538079</td>\n",
       "      <td>bing</td>\n",
       "      <td>afghane</td>\n",
       "      <td>2020-05-03 08:05:10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig, vorknöpfen, vo...</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}, {Synset(...</td>\n",
       "      <td>[modern, schelten, erwerben, akquirieren]</td>\n",
       "      <td>[{}, {}, {Synset(id=s123224, lexunits=hinzukau...</td>\n",
       "      <td>[hinzukaufen, buchen, aufkaufen, erkaufen, abk...</td>\n",
       "      <td>[tensor(-0.9030), tensor(0.7804), tensor(0.796...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-2.2221)</td>\n",
       "      <td>tensor(-1.7950)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4538080</td>\n",
       "      <td>ddg</td>\n",
       "      <td>bulgare</td>\n",
       "      <td>2020-05-03 08:05:10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.9550), tensor(-0.7253), tensor(-0.9...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-6.9096)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4538081</td>\n",
       "      <td>google_firefox</td>\n",
       "      <td>bulgare</td>\n",
       "      <td>2020-05-03 08:05:11.0</td>\n",
       "      <td>firefox</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s1477, lexunits=türkisch)], [Synse...</td>\n",
       "      <td>[türkisch, in, angesagt, kultig, trendig]</td>\n",
       "      <td>[{Synset(id=s1475, lexunits=eurasisch)}, {Syns...</td>\n",
       "      <td>[eurasisch, modern]</td>\n",
       "      <td>[{Synset(id=s78994, lexunits=anatolisch), Syns...</td>\n",
       "      <td>[anatolisch, trojanisch]</td>\n",
       "      <td>[tensor(0.7969), tensor(-0.4451), tensor(0.894...</td>\n",
       "      <td>[tensor(0.8932), tensor(0.6114), tensor(-0.846...</td>\n",
       "      <td>tensor(-3.0013)</td>\n",
       "      <td>tensor(2.4875)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4538082</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>russe</td>\n",
       "      <td>2020-05-03 08:05:12.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.9899), tensor(0.8730), tensor(-0.01...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-4.1248)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4538083</td>\n",
       "      <td>bing</td>\n",
       "      <td>russe</td>\n",
       "      <td>2020-05-03 08:05:12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig, online, verbun...</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}, {Synset(...</td>\n",
       "      <td>[modern, anwendungsbezogen]</td>\n",
       "      <td>[{}, {}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.1728), tensor(-0.9899), tensor(-0.9...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(1.8661)</td>\n",
       "      <td>tensor(1.6390)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4538084</td>\n",
       "      <td>ddg</td>\n",
       "      <td>afghane</td>\n",
       "      <td>2020-05-03 08:05:12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.8649), tensor(-0.9120), tensor(0.84...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-1.8887)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4538085</td>\n",
       "      <td>google_firefox</td>\n",
       "      <td>afghane</td>\n",
       "      <td>2020-05-03 08:05:13.0</td>\n",
       "      <td>firefox</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig, essen, futtern...</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}, {Synset(...</td>\n",
       "      <td>[modern, verzehren]</td>\n",
       "      <td>[{}, {Synset(id=s149627, lexunits=anfuttern), ...</td>\n",
       "      <td>[anfuttern, verfressen, verfuttern, hinuntersc...</td>\n",
       "      <td>[tensor(0.7558), tensor(-0.9794), tensor(0.611...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(0.7344)</td>\n",
       "      <td>tensor(2.2359)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4538086</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>serbe</td>\n",
       "      <td>2020-05-03 08:05:13.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig]</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}]</td>\n",
       "      <td>[modern]</td>\n",
       "      <td>[{}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.3980), tensor(0.7969), tensor(-0.10...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-1.1006)</td>\n",
       "      <td>tensor(1.5943)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4538087</td>\n",
       "      <td>bing</td>\n",
       "      <td>serbe</td>\n",
       "      <td>2020-05-03 08:05:13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig]</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}]</td>\n",
       "      <td>[modern]</td>\n",
       "      <td>[{}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.9050), tensor(0.6848), tensor(-0.00...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-6.4742)</td>\n",
       "      <td>tensor(1.5943)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4538088</td>\n",
       "      <td>ddg</td>\n",
       "      <td>russe</td>\n",
       "      <td>2020-05-03 08:05:13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.8906), tensor(-0.9832), tensor(0.86...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-1.0209)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4538089</td>\n",
       "      <td>ddg</td>\n",
       "      <td>serbe</td>\n",
       "      <td>2020-05-03 08:05:16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.7158), tensor(-0.3722), tensor(-0.6...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-6.4975)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4538090</td>\n",
       "      <td>google_firefox</td>\n",
       "      <td>russe</td>\n",
       "      <td>2020-05-03 08:05:15.0</td>\n",
       "      <td>firefox</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.9899), tensor(0.8730), tensor(-0.01...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-4.1248)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4538091</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>portugiese</td>\n",
       "      <td>2020-05-03 08:05:15.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.2120), tensor(0.4906), tensor(-0.86...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-0.7692)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4538092</td>\n",
       "      <td>bing</td>\n",
       "      <td>portugiese</td>\n",
       "      <td>2020-05-03 08:05:15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s736, lexunits=lieblich, liebreize...</td>\n",
       "      <td>[lieblich, liebreizend]</td>\n",
       "      <td>[{Synset(id=s734, lexunits=anziehend, attrakti...</td>\n",
       "      <td>[anziehend, attraktiv]</td>\n",
       "      <td>[{Synset(id=s96331, lexunits=hold)}]</td>\n",
       "      <td>[hold]</td>\n",
       "      <td>[tensor(-0.8670), tensor(-0.7942), tensor(0.82...</td>\n",
       "      <td>[tensor(0.9604), tensor(0.9315)]</td>\n",
       "      <td>tensor(1.6831)</td>\n",
       "      <td>tensor(1.8919)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4538093</td>\n",
       "      <td>ddg</td>\n",
       "      <td>portugiese</td>\n",
       "      <td>2020-05-03 08:05:17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(0.8827), tensor(-0.9292), tensor(0.790...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-0.1292)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4538094</td>\n",
       "      <td>bing</td>\n",
       "      <td>franzose</td>\n",
       "      <td>2020-05-03 08:05:17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.8971), tensor(-0.9508), tensor(-0.9...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-4.5161)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4538095</td>\n",
       "      <td>google_firefox</td>\n",
       "      <td>serbe</td>\n",
       "      <td>2020-05-03 08:05:17.0</td>\n",
       "      <td>firefox</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig]</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}]</td>\n",
       "      <td>[modern]</td>\n",
       "      <td>[{}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.3980), tensor(0.7969), tensor(-0.10...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-2.0192)</td>\n",
       "      <td>tensor(1.5943)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4538096</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>franzose</td>\n",
       "      <td>2020-05-03 08:05:17.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig]</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}]</td>\n",
       "      <td>[modern]</td>\n",
       "      <td>[{}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.9509), tensor(-0.8794), tensor(-0.9...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-3.4800)</td>\n",
       "      <td>tensor(1.5943)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4538097</td>\n",
       "      <td>ddg</td>\n",
       "      <td>franzose</td>\n",
       "      <td>2020-05-03 08:05:18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s1417, lexunits=englisch)]]</td>\n",
       "      <td>[englisch]</td>\n",
       "      <td>[{Synset(id=s1415, lexunits=britisch, großbrit...</td>\n",
       "      <td>[britisch, großbritannisch, britannisch]</td>\n",
       "      <td>[{Synset(id=s1419, lexunits=südenglisch), Syns...</td>\n",
       "      <td>[südenglisch, nordwestenglisch, mittelenglisch...</td>\n",
       "      <td>[tensor(-0.9488), tensor(-0.9412), tensor(-0.9...</td>\n",
       "      <td>[tensor(-0.8800)]</td>\n",
       "      <td>tensor(-5.8471)</td>\n",
       "      <td>tensor(-0.8800)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4538098</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>chinese</td>\n",
       "      <td>2020-05-03 08:05:19.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.2120), tensor(-0.4819), tensor(-0.3...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-2.0306)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4538099</td>\n",
       "      <td>bing</td>\n",
       "      <td>chinese</td>\n",
       "      <td>2020-05-03 08:05:18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig, reisen, vorknö...</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}, {Synset(...</td>\n",
       "      <td>[modern, Individuum spez, schelten, erwerben, ...</td>\n",
       "      <td>[{}, {Synset(id=s57903, lexunits=jetten), Syns...</td>\n",
       "      <td>[jetten, kutschieren, wegreisen, fortreisen, z...</td>\n",
       "      <td>[tensor(0.8184), tensor(0.6576), tensor(0.1248...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(0.7887)</td>\n",
       "      <td>tensor(-1.0711)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4538100</td>\n",
       "      <td>google_firefox</td>\n",
       "      <td>portugiese</td>\n",
       "      <td>2020-05-03 08:05:19.0</td>\n",
       "      <td>firefox</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.2120), tensor(0.4906), tensor(-0.86...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-0.7692)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4538101</td>\n",
       "      <td>ddg</td>\n",
       "      <td>chinese</td>\n",
       "      <td>2020-05-03 08:05:20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(0.9502), tensor(-0.0154), tensor(-0.86...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(4.4163)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4538102</td>\n",
       "      <td>google_firefox</td>\n",
       "      <td>franzose</td>\n",
       "      <td>2020-05-03 08:05:21.0</td>\n",
       "      <td>firefox</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s5333, lexunits=in, angesagt, kult...</td>\n",
       "      <td>[in, angesagt, kultig, trendig]</td>\n",
       "      <td>[{Synset(id=s5330, lexunits=modern)}]</td>\n",
       "      <td>[modern]</td>\n",
       "      <td>[{}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.9509), tensor(-0.8794), tensor(-0.9...</td>\n",
       "      <td>[tensor(0.6114), tensor(-0.8469), tensor(0.976...</td>\n",
       "      <td>tensor(-3.4800)</td>\n",
       "      <td>tensor(1.5943)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4538103</td>\n",
       "      <td>google_psy</td>\n",
       "      <td>jugoslawe</td>\n",
       "      <td>2020-05-03 08:05:20.0</td>\n",
       "      <td>psy-ab</td>\n",
       "      <td>de-DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>https://www.google.com/complete/search</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[[Synset(id=s61028, lexunits=essen, futtern, n...</td>\n",
       "      <td>[essen, futtern, nehmen]</td>\n",
       "      <td>[{Synset(id=s61025, lexunits=verzehren)}]</td>\n",
       "      <td>[verzehren]</td>\n",
       "      <td>[{Synset(id=s149627, lexunits=anfuttern), Syns...</td>\n",
       "      <td>[anfuttern, verfressen, verfuttern, hinuntersc...</td>\n",
       "      <td>[tensor(-0.9932), tensor(-0.9495), tensor(-0.4...</td>\n",
       "      <td>[tensor(0.8043), tensor(-0.9679), tensor(0.8051)]</td>\n",
       "      <td>tensor(-0.7525)</td>\n",
       "      <td>tensor(0.6416)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4538104</td>\n",
       "      <td>ddg</td>\n",
       "      <td>jugoslawe</td>\n",
       "      <td>2020-05-03 08:05:21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://duckduckgo.com/ac/</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.0058), tensor(0.0691), tensor(-0.04...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-0.5341)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4538105</td>\n",
       "      <td>bing</td>\n",
       "      <td>jugoslawe</td>\n",
       "      <td>2020-05-03 08:05:21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://api.bing.net/osjson.aspx</td>\n",
       "      <td>localhost</td>\n",
       "      <td>Köln</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tensor(-0.4855), tensor(-0.8548), tensor(-0.6...</td>\n",
       "      <td></td>\n",
       "      <td>tensor(-5.4380)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    suggestion_id           label   queryterm                   date   client  \\\n",
       "0         4538056      google_psy       türke  2020-05-03 08:05:01.0   psy-ab   \n",
       "1         4538057  google_firefox       türke  2020-05-03 08:05:01.0  firefox   \n",
       "2         4538058             ddg       türke  2020-05-03 08:05:01.0      NaN   \n",
       "3         4538059            bing       türke  2020-05-03 08:05:01.0      NaN   \n",
       "4         4538060      google_psy        pole  2020-05-03 08:05:02.0   psy-ab   \n",
       "5         4538061  google_firefox        pole  2020-05-03 08:05:02.0  firefox   \n",
       "6         4538062            bing        pole  2020-05-03 08:05:03.0      NaN   \n",
       "7         4538063             ddg        pole  2020-05-03 08:05:03.0      NaN   \n",
       "8         4538064      google_psy      rumäne  2020-05-03 08:05:03.0   psy-ab   \n",
       "9         4538065            bing      rumäne  2020-05-03 08:05:05.0      NaN   \n",
       "10        4538066  google_firefox      rumäne  2020-05-03 08:05:04.0  firefox   \n",
       "11        4538067            bing     grieche  2020-05-03 08:05:06.0      NaN   \n",
       "12        4538068             ddg      rumäne  2020-05-03 08:05:05.0      NaN   \n",
       "13        4538069      google_psy     grieche  2020-05-03 08:05:05.0   psy-ab   \n",
       "14        4538070            bing      kroate  2020-05-03 08:05:07.0      NaN   \n",
       "15        4538071  google_firefox     grieche  2020-05-03 08:05:07.0  firefox   \n",
       "16        4538072      google_psy      kroate  2020-05-03 08:05:08.0   psy-ab   \n",
       "17        4538073             ddg     grieche  2020-05-03 08:05:07.0      NaN   \n",
       "18        4538074      google_psy     bulgare  2020-05-03 08:05:09.0   psy-ab   \n",
       "19        4538075            bing     bulgare  2020-05-03 08:05:08.0      NaN   \n",
       "20        4538076             ddg      kroate  2020-05-03 08:05:09.0      NaN   \n",
       "21        4538077  google_firefox      kroate  2020-05-03 08:05:09.0  firefox   \n",
       "22        4538078      google_psy     afghane  2020-05-03 08:05:10.0   psy-ab   \n",
       "23        4538079            bing     afghane  2020-05-03 08:05:10.0      NaN   \n",
       "24        4538080             ddg     bulgare  2020-05-03 08:05:10.0      NaN   \n",
       "25        4538081  google_firefox     bulgare  2020-05-03 08:05:11.0  firefox   \n",
       "26        4538082      google_psy       russe  2020-05-03 08:05:12.0   psy-ab   \n",
       "27        4538083            bing       russe  2020-05-03 08:05:12.0      NaN   \n",
       "28        4538084             ddg     afghane  2020-05-03 08:05:12.0      NaN   \n",
       "29        4538085  google_firefox     afghane  2020-05-03 08:05:13.0  firefox   \n",
       "30        4538086      google_psy       serbe  2020-05-03 08:05:13.0   psy-ab   \n",
       "31        4538087            bing       serbe  2020-05-03 08:05:13.0      NaN   \n",
       "32        4538088             ddg       russe  2020-05-03 08:05:13.0      NaN   \n",
       "33        4538089             ddg       serbe  2020-05-03 08:05:16.0      NaN   \n",
       "34        4538090  google_firefox       russe  2020-05-03 08:05:15.0  firefox   \n",
       "35        4538091      google_psy  portugiese  2020-05-03 08:05:15.0   psy-ab   \n",
       "36        4538092            bing  portugiese  2020-05-03 08:05:15.0      NaN   \n",
       "37        4538093             ddg  portugiese  2020-05-03 08:05:17.0      NaN   \n",
       "38        4538094            bing    franzose  2020-05-03 08:05:17.0      NaN   \n",
       "39        4538095  google_firefox       serbe  2020-05-03 08:05:17.0  firefox   \n",
       "40        4538096      google_psy    franzose  2020-05-03 08:05:17.0   psy-ab   \n",
       "41        4538097             ddg    franzose  2020-05-03 08:05:18.0      NaN   \n",
       "42        4538098      google_psy     chinese  2020-05-03 08:05:19.0   psy-ab   \n",
       "43        4538099            bing     chinese  2020-05-03 08:05:18.0      NaN   \n",
       "44        4538100  google_firefox  portugiese  2020-05-03 08:05:19.0  firefox   \n",
       "45        4538101             ddg     chinese  2020-05-03 08:05:20.0      NaN   \n",
       "46        4538102  google_firefox    franzose  2020-05-03 08:05:21.0  firefox   \n",
       "47        4538103      google_psy   jugoslawe  2020-05-03 08:05:20.0   psy-ab   \n",
       "48        4538104             ddg   jugoslawe  2020-05-03 08:05:21.0      NaN   \n",
       "49        4538105            bing   jugoslawe  2020-05-03 08:05:21.0      NaN   \n",
       "\n",
       "     lang geolocation                                     url   proxy_ip  \\\n",
       "0   de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "1   de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "2     NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "3     NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "4   de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "5   de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "6     NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "7     NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "8   de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "9     NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "10  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "11    NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "12    NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "13  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "14    NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "15  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "16  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "17    NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "18  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "19    NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "20    NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "21  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "22  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "23    NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "24    NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "25  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "26  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "27    NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "28    NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "29  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "30  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "31    NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "32    NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "33    NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "34  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "35  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "36    NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "37    NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "38    NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "39  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "40  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "41    NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "42  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "43    NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "44  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "45    NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "46  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "47  de-DE          DE  https://www.google.com/complete/search  localhost   \n",
       "48    NaN         NaN              https://duckduckgo.com/ac/  localhost   \n",
       "49    NaN         NaN         http://api.bing.net/osjson.aspx  localhost   \n",
       "\n",
       "   proxy_city  ...                                        synsets_ger  \\\n",
       "0        Köln  ...                                                 []   \n",
       "1        Köln  ...                                                 []   \n",
       "2        Köln  ...                                                 []   \n",
       "3        Köln  ...  [[Synset(id=s55185, lexunits=zurücktreten), Sy...   \n",
       "4        Köln  ...  [[Synset(id=s66916, lexunits=polen)], [Synset(...   \n",
       "5        Köln  ...  [[Synset(id=s66916, lexunits=polen)], [Synset(...   \n",
       "6        Köln  ...  [[Synset(id=s66916, lexunits=polen)], [Synset(...   \n",
       "7        Köln  ...                                                 []   \n",
       "8        Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "9        Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "10       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "11       Köln  ...                                                 []   \n",
       "12       Köln  ...                                                 []   \n",
       "13       Köln  ...                                                 []   \n",
       "14       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "15       Köln  ...                                                 []   \n",
       "16       Köln  ...                                                 []   \n",
       "17       Köln  ...                                                 []   \n",
       "18       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "19       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "20       Köln  ...                                                 []   \n",
       "21       Köln  ...                                                 []   \n",
       "22       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "23       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "24       Köln  ...                                                 []   \n",
       "25       Köln  ...  [[Synset(id=s1477, lexunits=türkisch)], [Synse...   \n",
       "26       Köln  ...                                                 []   \n",
       "27       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "28       Köln  ...                                                 []   \n",
       "29       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "30       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "31       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "32       Köln  ...                                                 []   \n",
       "33       Köln  ...                                                 []   \n",
       "34       Köln  ...                                                 []   \n",
       "35       Köln  ...                                                 []   \n",
       "36       Köln  ...  [[Synset(id=s736, lexunits=lieblich, liebreize...   \n",
       "37       Köln  ...                                                 []   \n",
       "38       Köln  ...                                                 []   \n",
       "39       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "40       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "41       Köln  ...            [[Synset(id=s1417, lexunits=englisch)]]   \n",
       "42       Köln  ...                                                 []   \n",
       "43       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "44       Köln  ...                                                 []   \n",
       "45       Köln  ...                                                 []   \n",
       "46       Köln  ...  [[Synset(id=s5333, lexunits=in, angesagt, kult...   \n",
       "47       Köln  ...  [[Synset(id=s61028, lexunits=essen, futtern, n...   \n",
       "48       Köln  ...                                                 []   \n",
       "49       Köln  ...                                                 []   \n",
       "\n",
       "                                             lexunits  \\\n",
       "0                                                  []   \n",
       "1                                                  []   \n",
       "2                                                  []   \n",
       "3   [zurücktreten, zurücktreten, zurücktreten, zur...   \n",
       "4                                  [polen, polemisch]   \n",
       "5                                  [polen, polemisch]   \n",
       "6                                [polen, italienisch]   \n",
       "7                                                  []   \n",
       "8   [in, angesagt, kultig, trendig, aussehen, auss...   \n",
       "9   [in, angesagt, kultig, trendig, funktionieren,...   \n",
       "10  [in, angesagt, kultig, trendig, aussehen, auss...   \n",
       "11                                                 []   \n",
       "12                                                 []   \n",
       "13                                                 []   \n",
       "14  [in, angesagt, kultig, trendig, plündern, abrä...   \n",
       "15                                                 []   \n",
       "16                                                 []   \n",
       "17                                                 []   \n",
       "18          [in, angesagt, kultig, trendig, türkisch]   \n",
       "19                    [in, angesagt, kultig, trendig]   \n",
       "20                                                 []   \n",
       "21                                                 []   \n",
       "22  [in, angesagt, kultig, trendig, essen, futtern...   \n",
       "23  [in, angesagt, kultig, trendig, vorknöpfen, vo...   \n",
       "24                                                 []   \n",
       "25          [türkisch, in, angesagt, kultig, trendig]   \n",
       "26                                                 []   \n",
       "27  [in, angesagt, kultig, trendig, online, verbun...   \n",
       "28                                                 []   \n",
       "29  [in, angesagt, kultig, trendig, essen, futtern...   \n",
       "30                    [in, angesagt, kultig, trendig]   \n",
       "31                    [in, angesagt, kultig, trendig]   \n",
       "32                                                 []   \n",
       "33                                                 []   \n",
       "34                                                 []   \n",
       "35                                                 []   \n",
       "36                            [lieblich, liebreizend]   \n",
       "37                                                 []   \n",
       "38                                                 []   \n",
       "39                    [in, angesagt, kultig, trendig]   \n",
       "40                    [in, angesagt, kultig, trendig]   \n",
       "41                                         [englisch]   \n",
       "42                                                 []   \n",
       "43  [in, angesagt, kultig, trendig, reisen, vorknö...   \n",
       "44                                                 []   \n",
       "45                                                 []   \n",
       "46                    [in, angesagt, kultig, trendig]   \n",
       "47                           [essen, futtern, nehmen]   \n",
       "48                                                 []   \n",
       "49                                                 []   \n",
       "\n",
       "                                            hypernyms  \\\n",
       "0                                                  []   \n",
       "1                                                  []   \n",
       "2                                                  []   \n",
       "3   [{Synset(id=s55172, lexunits=aufgeben, verzich...   \n",
       "4   [{Synset(id=s56823, lexunits=anschließen)}, {S...   \n",
       "5   [{Synset(id=s56823, lexunits=anschließen)}, {S...   \n",
       "6   [{Synset(id=s56823, lexunits=anschließen)}, {S...   \n",
       "7                                                  []   \n",
       "8   [{Synset(id=s5330, lexunits=modern)}, {Synset(...   \n",
       "9   [{Synset(id=s5330, lexunits=modern)}, {Synset(...   \n",
       "10  [{Synset(id=s5330, lexunits=modern)}, {Synset(...   \n",
       "11                                                 []   \n",
       "12                                                 []   \n",
       "13                                                 []   \n",
       "14  [{Synset(id=s5330, lexunits=modern)}, {Synset(...   \n",
       "15                                                 []   \n",
       "16                                                 []   \n",
       "17                                                 []   \n",
       "18  [{Synset(id=s5330, lexunits=modern)}, {Synset(...   \n",
       "19              [{Synset(id=s5330, lexunits=modern)}]   \n",
       "20                                                 []   \n",
       "21                                                 []   \n",
       "22  [{Synset(id=s5330, lexunits=modern)}, {Synset(...   \n",
       "23  [{Synset(id=s5330, lexunits=modern)}, {Synset(...   \n",
       "24                                                 []   \n",
       "25  [{Synset(id=s1475, lexunits=eurasisch)}, {Syns...   \n",
       "26                                                 []   \n",
       "27  [{Synset(id=s5330, lexunits=modern)}, {Synset(...   \n",
       "28                                                 []   \n",
       "29  [{Synset(id=s5330, lexunits=modern)}, {Synset(...   \n",
       "30              [{Synset(id=s5330, lexunits=modern)}]   \n",
       "31              [{Synset(id=s5330, lexunits=modern)}]   \n",
       "32                                                 []   \n",
       "33                                                 []   \n",
       "34                                                 []   \n",
       "35                                                 []   \n",
       "36  [{Synset(id=s734, lexunits=anziehend, attrakti...   \n",
       "37                                                 []   \n",
       "38                                                 []   \n",
       "39              [{Synset(id=s5330, lexunits=modern)}]   \n",
       "40              [{Synset(id=s5330, lexunits=modern)}]   \n",
       "41  [{Synset(id=s1415, lexunits=britisch, großbrit...   \n",
       "42                                                 []   \n",
       "43  [{Synset(id=s5330, lexunits=modern)}, {Synset(...   \n",
       "44                                                 []   \n",
       "45                                                 []   \n",
       "46              [{Synset(id=s5330, lexunits=modern)}]   \n",
       "47          [{Synset(id=s61025, lexunits=verzehren)}]   \n",
       "48                                                 []   \n",
       "49                                                 []   \n",
       "\n",
       "                                   lexunits_hypernyms  \\\n",
       "0                                                  []   \n",
       "1                                                  []   \n",
       "2                                                  []   \n",
       "3   [aufgeben, verzichten, versagen, ausscheiden, ...   \n",
       "4   [anschließen, stilistisch, provozierend, provo...   \n",
       "5   [anschließen, stilistisch, provozierend, provo...   \n",
       "6           [anschließen, südeuropäisch, südländisch]   \n",
       "7                                                  []   \n",
       "8       [modern, sein, suchen, sein, fehlinformieren]   \n",
       "9   [modern, passieren, ereignen, denken, befassen...   \n",
       "10      [modern, sein, suchen, sein, fehlinformieren]   \n",
       "11                                                 []   \n",
       "12                                                 []   \n",
       "13                                                 []   \n",
       "14          [modern, wegnehmen, abnehmen, fortnehmen]   \n",
       "15                                                 []   \n",
       "16                                                 []   \n",
       "17                                                 []   \n",
       "18                                [modern, eurasisch]   \n",
       "19                                           [modern]   \n",
       "20                                                 []   \n",
       "21                                                 []   \n",
       "22                                [modern, verzehren]   \n",
       "23          [modern, schelten, erwerben, akquirieren]   \n",
       "24                                                 []   \n",
       "25                                [eurasisch, modern]   \n",
       "26                                                 []   \n",
       "27                        [modern, anwendungsbezogen]   \n",
       "28                                                 []   \n",
       "29                                [modern, verzehren]   \n",
       "30                                           [modern]   \n",
       "31                                           [modern]   \n",
       "32                                                 []   \n",
       "33                                                 []   \n",
       "34                                                 []   \n",
       "35                                                 []   \n",
       "36                             [anziehend, attraktiv]   \n",
       "37                                                 []   \n",
       "38                                                 []   \n",
       "39                                           [modern]   \n",
       "40                                           [modern]   \n",
       "41           [britisch, großbritannisch, britannisch]   \n",
       "42                                                 []   \n",
       "43  [modern, Individuum spez, schelten, erwerben, ...   \n",
       "44                                                 []   \n",
       "45                                                 []   \n",
       "46                                           [modern]   \n",
       "47                                        [verzehren]   \n",
       "48                                                 []   \n",
       "49                                                 []   \n",
       "\n",
       "                                             hyponyms  \\\n",
       "0                                                  []   \n",
       "1                                                  []   \n",
       "2                                                  []   \n",
       "3   [{}, {Synset(id=s80699, lexunits=demissioniere...   \n",
       "4         [{Synset(id=s80471, lexunits=umpolen)}, {}]   \n",
       "5         [{Synset(id=s80471, lexunits=umpolen)}, {}]   \n",
       "6   [{Synset(id=s80471, lexunits=umpolen)}, {Synse...   \n",
       "7                                                  []   \n",
       "8   [{}, {Synset(id=s52138, lexunits=ausnehmen), S...   \n",
       "9   [{}, {Synset(id=s105884, lexunits=nachgehen), ...   \n",
       "10  [{}, {Synset(id=s52138, lexunits=ausnehmen), S...   \n",
       "11                                                 []   \n",
       "12                                                 []   \n",
       "13                                                 []   \n",
       "14  [{}, {Synset(id=s52520, lexunits=marodieren), ...   \n",
       "15                                                 []   \n",
       "16                                                 []   \n",
       "17                                                 []   \n",
       "18  [{}, {Synset(id=s78994, lexunits=anatolisch), ...   \n",
       "19                                               [{}]   \n",
       "20                                                 []   \n",
       "21                                                 []   \n",
       "22  [{}, {Synset(id=s149627, lexunits=anfuttern), ...   \n",
       "23  [{}, {}, {Synset(id=s123224, lexunits=hinzukau...   \n",
       "24                                                 []   \n",
       "25  [{Synset(id=s78994, lexunits=anatolisch), Syns...   \n",
       "26                                                 []   \n",
       "27                                           [{}, {}]   \n",
       "28                                                 []   \n",
       "29  [{}, {Synset(id=s149627, lexunits=anfuttern), ...   \n",
       "30                                               [{}]   \n",
       "31                                               [{}]   \n",
       "32                                                 []   \n",
       "33                                                 []   \n",
       "34                                                 []   \n",
       "35                                                 []   \n",
       "36               [{Synset(id=s96331, lexunits=hold)}]   \n",
       "37                                                 []   \n",
       "38                                                 []   \n",
       "39                                               [{}]   \n",
       "40                                               [{}]   \n",
       "41  [{Synset(id=s1419, lexunits=südenglisch), Syns...   \n",
       "42                                                 []   \n",
       "43  [{}, {Synset(id=s57903, lexunits=jetten), Syns...   \n",
       "44                                                 []   \n",
       "45                                                 []   \n",
       "46                                               [{}]   \n",
       "47  [{Synset(id=s149627, lexunits=anfuttern), Syns...   \n",
       "48                                                 []   \n",
       "49                                                 []   \n",
       "\n",
       "                                    lexunits_hyponyms  \\\n",
       "0                                                  []   \n",
       "1                                                  []   \n",
       "2                                                  []   \n",
       "3   [demissionieren, beitragsfrei, gebührenfrei, g...   \n",
       "4                                           [umpolen]   \n",
       "5                                           [umpolen]   \n",
       "6   [umpolen, mittelitalienisch, süditalienisch, n...   \n",
       "7                                                  []   \n",
       "8   [ausnehmen, wirken, schauen, rumlügen, heuchel...   \n",
       "9   [nachgehen, vorgehen, leerlaufen, zuarbeiten, ...   \n",
       "10  [ausnehmen, wirken, schauen, rumlügen, heuchel...   \n",
       "11                                                 []   \n",
       "12                                                 []   \n",
       "13                                                 []   \n",
       "14                             [marodieren, fleddern]   \n",
       "15                                                 []   \n",
       "16                                                 []   \n",
       "17                                                 []   \n",
       "18                           [anatolisch, trojanisch]   \n",
       "19                                                 []   \n",
       "20                                                 []   \n",
       "21                                                 []   \n",
       "22  [anfuttern, verfressen, verfuttern, hinuntersc...   \n",
       "23  [hinzukaufen, buchen, aufkaufen, erkaufen, abk...   \n",
       "24                                                 []   \n",
       "25                           [anatolisch, trojanisch]   \n",
       "26                                                 []   \n",
       "27                                                 []   \n",
       "28                                                 []   \n",
       "29  [anfuttern, verfressen, verfuttern, hinuntersc...   \n",
       "30                                                 []   \n",
       "31                                                 []   \n",
       "32                                                 []   \n",
       "33                                                 []   \n",
       "34                                                 []   \n",
       "35                                                 []   \n",
       "36                                             [hold]   \n",
       "37                                                 []   \n",
       "38                                                 []   \n",
       "39                                                 []   \n",
       "40                                                 []   \n",
       "41  [südenglisch, nordwestenglisch, mittelenglisch...   \n",
       "42                                                 []   \n",
       "43  [jetten, kutschieren, wegreisen, fortreisen, z...   \n",
       "44                                                 []   \n",
       "45                                                 []   \n",
       "46                                                 []   \n",
       "47  [anfuttern, verfressen, verfuttern, hinuntersc...   \n",
       "48                                                 []   \n",
       "49                                                 []   \n",
       "\n",
       "                            tokens_suggestions_tensor  \\\n",
       "0   [tensor(-0.8585), tensor(0.9375), tensor(-0.01...   \n",
       "1   [tensor(-0.8585), tensor(-0.0100), tensor(-0.8...   \n",
       "2   [tensor(-0.8837), tensor(-0.6965), tensor(0.63...   \n",
       "3   [tensor(-0.8585), tensor(0.9375), tensor(0.433...   \n",
       "4   [tensor(-0.7592), tensor(-0.8762), tensor(0.81...   \n",
       "5   [tensor(-0.7592), tensor(-0.8762), tensor(0.81...   \n",
       "6   [tensor(0.1253), tensor(-0.8762), tensor(0.940...   \n",
       "7   [tensor(0.8301), tensor(0.7862), tensor(0.1622...   \n",
       "8   [tensor(0.9491), tensor(0.7781), tensor(-0.949...   \n",
       "9   [tensor(0.6576), tensor(0.7320), tensor(-0.841...   \n",
       "10  [tensor(0.9491), tensor(0.7781), tensor(-0.949...   \n",
       "11  [tensor(0.9799), tensor(-0.0652), tensor(0.937...   \n",
       "12  [tensor(-0.8853), tensor(-0.8547), tensor(0.57...   \n",
       "13  [tensor(-0.8435), tensor(0.9319), tensor(0.786...   \n",
       "14  [tensor(-0.8276), tensor(-0.9004), tensor(0.80...   \n",
       "15  [tensor(-0.8435), tensor(0.9319), tensor(0.786...   \n",
       "16  [tensor(0.0378), tensor(-0.9932), tensor(0.836...   \n",
       "17  [tensor(0.7365), tensor(0.7808), tensor(0.7474...   \n",
       "18  [tensor(-0.9799), tensor(-0.9274), tensor(0.68...   \n",
       "19  [tensor(0.7969), tensor(0.8885), tensor(-0.979...   \n",
       "20  [tensor(0.4713), tensor(-0.6715), tensor(0.217...   \n",
       "21  [tensor(0.0378), tensor(-0.9932), tensor(0.836...   \n",
       "22  [tensor(0.7558), tensor(-0.9794), tensor(0.611...   \n",
       "23  [tensor(-0.9030), tensor(0.7804), tensor(0.796...   \n",
       "24  [tensor(-0.9550), tensor(-0.7253), tensor(-0.9...   \n",
       "25  [tensor(0.7969), tensor(-0.4451), tensor(0.894...   \n",
       "26  [tensor(-0.9899), tensor(0.8730), tensor(-0.01...   \n",
       "27  [tensor(-0.1728), tensor(-0.9899), tensor(-0.9...   \n",
       "28  [tensor(-0.8649), tensor(-0.9120), tensor(0.84...   \n",
       "29  [tensor(0.7558), tensor(-0.9794), tensor(0.611...   \n",
       "30  [tensor(-0.3980), tensor(0.7969), tensor(-0.10...   \n",
       "31  [tensor(-0.9050), tensor(0.6848), tensor(-0.00...   \n",
       "32  [tensor(-0.8906), tensor(-0.9832), tensor(0.86...   \n",
       "33  [tensor(-0.7158), tensor(-0.3722), tensor(-0.6...   \n",
       "34  [tensor(-0.9899), tensor(0.8730), tensor(-0.01...   \n",
       "35  [tensor(-0.2120), tensor(0.4906), tensor(-0.86...   \n",
       "36  [tensor(-0.8670), tensor(-0.7942), tensor(0.82...   \n",
       "37  [tensor(0.8827), tensor(-0.9292), tensor(0.790...   \n",
       "38  [tensor(-0.8971), tensor(-0.9508), tensor(-0.9...   \n",
       "39  [tensor(-0.3980), tensor(0.7969), tensor(-0.10...   \n",
       "40  [tensor(-0.9509), tensor(-0.8794), tensor(-0.9...   \n",
       "41  [tensor(-0.9488), tensor(-0.9412), tensor(-0.9...   \n",
       "42  [tensor(-0.2120), tensor(-0.4819), tensor(-0.3...   \n",
       "43  [tensor(0.8184), tensor(0.6576), tensor(0.1248...   \n",
       "44  [tensor(-0.2120), tensor(0.4906), tensor(-0.86...   \n",
       "45  [tensor(0.9502), tensor(-0.0154), tensor(-0.86...   \n",
       "46  [tensor(-0.9509), tensor(-0.8794), tensor(-0.9...   \n",
       "47  [tensor(-0.9932), tensor(-0.9495), tensor(-0.4...   \n",
       "48  [tensor(-0.0058), tensor(0.0691), tensor(-0.04...   \n",
       "49  [tensor(-0.4855), tensor(-0.8548), tensor(-0.6...   \n",
       "\n",
       "                                      lexunits_tensor  \\\n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                       \n",
       "3   [tensor(-0.9261), tensor(-0.9261), tensor(-0.9...   \n",
       "4                  [tensor(-0.8216), tensor(-0.8856)]   \n",
       "5                  [tensor(-0.8216), tensor(-0.8856)]   \n",
       "6                   [tensor(-0.8196), tensor(0.6341)]   \n",
       "7                                                       \n",
       "8   [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "9   [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "10  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "11                                                      \n",
       "12                                                      \n",
       "13                                                      \n",
       "14  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "15                                                      \n",
       "16                                                      \n",
       "17                                                      \n",
       "18  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "19  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "20                                                      \n",
       "21                                                      \n",
       "22  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "23  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "24                                                      \n",
       "25  [tensor(0.8932), tensor(0.6114), tensor(-0.846...   \n",
       "26                                                      \n",
       "27  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "28                                                      \n",
       "29  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "30  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "31  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "32                                                      \n",
       "33                                                      \n",
       "34                                                      \n",
       "35                                                      \n",
       "36                   [tensor(0.9604), tensor(0.9315)]   \n",
       "37                                                      \n",
       "38                                                      \n",
       "39  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "40  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "41                                  [tensor(-0.8800)]   \n",
       "42                                                      \n",
       "43  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "44                                                      \n",
       "45                                                      \n",
       "46  [tensor(0.6114), tensor(-0.8469), tensor(0.976...   \n",
       "47  [tensor(0.8043), tensor(-0.9679), tensor(0.8051)]   \n",
       "48                                                      \n",
       "49                                                      \n",
       "\n",
       "   tokens_suggestions_tensor_sum lexunits_tensor_sum  \n",
       "0                 tensor(1.6650)                None  \n",
       "1                 tensor(0.5884)                None  \n",
       "2                tensor(-1.3746)                None  \n",
       "3                 tensor(3.1092)     tensor(-2.4034)  \n",
       "4                tensor(-2.8080)     tensor(-1.7072)  \n",
       "5                tensor(-3.7279)     tensor(-1.7072)  \n",
       "6                tensor(-0.6651)     tensor(-0.1856)  \n",
       "7                 tensor(3.4194)                None  \n",
       "8                tensor(-4.1615)     tensor(-3.5504)  \n",
       "9                tensor(-1.1195)      tensor(0.9746)  \n",
       "10               tensor(-4.1615)     tensor(-3.5504)  \n",
       "11                tensor(0.0468)                None  \n",
       "12               tensor(-4.7053)                None  \n",
       "13                tensor(3.8756)                None  \n",
       "14               tensor(-6.0244)     tensor(-1.1680)  \n",
       "15                tensor(3.8756)                None  \n",
       "16               tensor(-1.3039)                None  \n",
       "17                tensor(0.4034)                None  \n",
       "18               tensor(-3.8855)      tensor(2.4875)  \n",
       "19               tensor(-3.7403)      tensor(1.5943)  \n",
       "20               tensor(-2.8406)                None  \n",
       "21               tensor(-1.3039)                None  \n",
       "22                tensor(0.7344)      tensor(2.2359)  \n",
       "23               tensor(-2.2221)     tensor(-1.7950)  \n",
       "24               tensor(-6.9096)                None  \n",
       "25               tensor(-3.0013)      tensor(2.4875)  \n",
       "26               tensor(-4.1248)                None  \n",
       "27                tensor(1.8661)      tensor(1.6390)  \n",
       "28               tensor(-1.8887)                None  \n",
       "29                tensor(0.7344)      tensor(2.2359)  \n",
       "30               tensor(-1.1006)      tensor(1.5943)  \n",
       "31               tensor(-6.4742)      tensor(1.5943)  \n",
       "32               tensor(-1.0209)                None  \n",
       "33               tensor(-6.4975)                None  \n",
       "34               tensor(-4.1248)                None  \n",
       "35               tensor(-0.7692)                None  \n",
       "36                tensor(1.6831)      tensor(1.8919)  \n",
       "37               tensor(-0.1292)                None  \n",
       "38               tensor(-4.5161)                None  \n",
       "39               tensor(-2.0192)      tensor(1.5943)  \n",
       "40               tensor(-3.4800)      tensor(1.5943)  \n",
       "41               tensor(-5.8471)     tensor(-0.8800)  \n",
       "42               tensor(-2.0306)                None  \n",
       "43                tensor(0.7887)     tensor(-1.0711)  \n",
       "44               tensor(-0.7692)                None  \n",
       "45                tensor(4.4163)                None  \n",
       "46               tensor(-3.4800)      tensor(1.5943)  \n",
       "47               tensor(-0.7525)      tensor(0.6416)  \n",
       "48               tensor(-0.5341)                None  \n",
       "49               tensor(-5.4380)                None  \n",
       "\n",
       "[50 rows x 29 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum all tensors in a line for tokens_suggestions and lexunits\n",
    "def sum_tensors (tensors):\n",
    "    if tensors != '':\n",
    "        return sum(tensors)\n",
    "    #sum_tensors = sum(tensors)\n",
    "df['tokens_suggestions_tensor_sum'] = df.tokens_suggestions_tensor.swifter.apply(lambda row: sum_tensors(row))\n",
    "df['lexunits_tensor_sum'] = df.lexunits_tensor.swifter.apply(lambda row: sum_tensors(row))\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender\n",
    "df.loc[df.stemmed_words=='manner','stemmed_words'] = 'Mann'\n",
    "df.loc[df.stemmed_words=='frauen','stemmed_words'] = 'Frau'\n",
    "df.loc[df.stemmed_words=='mann','stemmed_words'] = 'Mann'\n",
    "df.loc[df.stemmed_words=='frau','stemmed_words'] = 'Frau'\n",
    "#minorities - cultural, linguistic and historic criteria\n",
    "df.loc[df.stemmed_words=='rom','stemmed_words'] = 'Roma'\n",
    "df.loc[df.stemmed_words=='roma','stemmed_words'] = 'Roma'\n",
    "df.loc[df.stemmed_words=='sinto','stemmed_words'] = 'Sinti'\n",
    "df.loc[df.stemmed_words=='sinti','stemmed_words'] = 'Sinti'\n",
    "df.loc[df.stemmed_words=='fluchtling','stemmed_words'] = 'Migrant'\n",
    "df.loc[df.stemmed_words=='illegaler','stemmed_words'] = 'Migrant'\n",
    "df.loc[df.stemmed_words=='illegal','stemmed_words'] = 'Migrant'\n",
    "df.loc[df.stemmed_words=='migra','stemmed_words'] = 'Migrant'\n",
    "df.loc[df.stemmed_words=='immigra','stemmed_words'] = 'Migrant'\n",
    "df.loc[df.stemmed_words=='arab','stemmed_words'] = 'arabisch'\n",
    "df.loc[df.stemmed_words=='kurd','stemmed_words'] = 'kurdisch'\n",
    "df.loc[df.stemmed_words=='jugoslawe','stemmed_words'] = 'jugoslawisch'\n",
    "df.loc[df.stemmed_words=='jugoslaw','stemmed_words'] = 'jugoslawisch'\n",
    "#skin color:\n",
    "df.loc[df.stemmed_words=='schwarzer','stemmed_words'] = 'schwarz'\n",
    "df.loc[df.stemmed_words=='weiss','stemmed_words'] = 'weiß'\n",
    "#population group - geografic or national\n",
    "#Europa\n",
    "df.loc[df.stemmed_words=='europa','stemmed_words'] = 'europäisch'\n",
    "#Westeuropa\n",
    "df.loc[df.stemmed_words=='franzo','stemmed_words'] = 'französisch'\n",
    "df.loc[df.stemmed_words=='franzosisch','stemmed_words'] = 'französisch'\n",
    "df.loc[df.stemmed_words=='engla','stemmed_words'] = 'englisch'\n",
    "df.loc[df.stemmed_words=='niederla','stemmed_words'] = 'niederländisch'\n",
    "df.loc[df.stemmed_words=='niederlandisch','stemmed_words'] = 'niederländisch'\n",
    "df.loc[df.stemmed_words=='holla','stemmed_words'] = 'holländisch'\n",
    "df.loc[df.stemmed_words=='hollandisch','stemmed_words'] = 'holländisch'\n",
    "#Mitteleuropa (Central Europa)\n",
    "df.loc[df.stemmed_words=='deutscher','stemmed_words'] = 'deutsch'\n",
    "df.loc[df.stemmed_words=='pol','stemmed_words'] = 'polnisch'\n",
    "df.loc[df.stemmed_words=='osterreich','stemmed_words'] = 'österreichisch'\n",
    "df.loc[df.stemmed_words=='osterreichisch','stemmed_words'] = 'österreichisch'\n",
    "df.loc[df.stemmed_words=='schweiz','stemmed_words'] = 'schweizerisch'\n",
    "#Osteuropa\n",
    "df.loc[df.stemmed_words=='russ','stemmed_words'] = 'russisch'\n",
    "df.loc[df.stemmed_words=='ukrai','stemmed_words'] = 'ukrainisch'\n",
    "#Südeuropa\n",
    "df.loc[df.stemmed_words=='italie','stemmed_words'] = 'italienisch'\n",
    "df.loc[df.stemmed_words=='portugie','stemmed_words'] = 'portugiesisch'\n",
    "df.loc[df.stemmed_words=='spanier','stemmed_words'] = 'spanisch'\n",
    "#Südosteuropa\n",
    "df.loc[df.stemmed_words=='griech','stemmed_words'] = 'griechisch'\n",
    "df.loc[df.stemmed_words=='kroa','stemmed_words'] = 'kroatisch'\n",
    "df.loc[df.stemmed_words=='rumanisch','stemmed_words'] = 'rumänisch'\n",
    "df.loc[df.stemmed_words=='ruma','stemmed_words'] = 'rumänisch'\n",
    "df.loc[df.stemmed_words=='serb','stemmed_words'] = 'serbisch'\n",
    "df.loc[df.stemmed_words=='bosnier','stemmed_words'] = 'bosnisch'\n",
    "df.loc[df.stemmed_words=='bulgar','stemmed_words'] = 'bulgarisch'\n",
    "#df.loc[df.stemmed_words=='türke','stemmed_words'] = 'türkisch'\n",
    "\n",
    "#Afrika\n",
    "df.loc[df.stemmed_words=='afrika','stemmed_words'] = 'afrikanisch'\n",
    "#Amerika\n",
    "df.loc[df.stemmed_words=='amerika','stemmed_words'] = 'amerikanisch'\n",
    "#Asien\n",
    "df.loc[df.stemmed_words=='asia','stemmed_words'] = 'asiatisch'\n",
    "#Voderasien\n",
    "df.loc[df.stemmed_words=='ira','stemmed_words'] = 'iranisch'\n",
    "df.loc[df.stemmed_words=='irak','stemmed_words'] = 'irakisch'\n",
    "df.loc[df.stemmed_words=='syrer','stemmed_words'] = 'syrisch'\n",
    "df.loc[df.stemmed_words=='turk','stemmed_words'] = 'türkisch'\n",
    "df.loc[df.stemmed_words=='turkisch','stemmed_words'] = 'türkisch'\n",
    "#Zentralasien\n",
    "df.loc[df.stemmed_words=='afgha','stemmed_words'] = 'afghanisch'\n",
    "#Ostasien\n",
    "df.loc[df.stemmed_words=='chi','stemmed_words'] = 'chinesisch'\n",
    "#Südasien\n",
    "df.loc[df.stemmed_words=='inder','stemmed_words'] = 'indisch'\n",
    "df.loc[df.stemmed_words=='pakista','stemmed_words'] = 'pakistanisch'\n",
    "#Südostasien\n",
    "df.loc[df.stemmed_words=='vietnam','stemmed_words'] = 'vietnamesisch'\n",
    "df.loc[df.stemmed_words=='israeli','stemmed_words'] = 'israelisch'\n",
    "#religious community\n",
    "df.loc[df.stemmed_words=='agnostik','stemmed_words'] = 'Agnostizismus'\n",
    "df.loc[df.stemmed_words=='agnostizismu','stemmed_words'] = 'Agnostizismus'\n",
    "df.loc[df.stemmed_words=='agnostisch','stemmed_words'] = 'Agnostizismus'\n",
    "df.loc[df.stemmed_words=='atheismu','stemmed_words'] = 'Atheismus'\n",
    "df.loc[df.stemmed_words=='atheistisch','stemmed_words'] = 'Atheismus'\n",
    "df.loc[df.stemmed_words=='atheti','stemmed_words'] = 'Atheismus'\n",
    "df.loc[df.stemmed_words=='athei','stemmed_words'] = 'Atheismus'\n",
    "df.loc[df.stemmed_words=='christentum','stemmed_words'] = 'Christentum'\n",
    "df.loc[df.stemmed_words=='chri','stemmed_words'] = 'Christentum'\n",
    "df.loc[df.stemmed_words=='christlich','stemmed_words'] = 'Christentum'\n",
    "df.loc[df.stemmed_words=='evangelisch','stemmed_words'] = 'Evangelikalismus'\n",
    "df.loc[df.stemmed_words=='jud','stemmed_words'] = 'Judentum'\n",
    "df.loc[df.stemmed_words=='judisch','stemmed_words'] = 'Judentum'\n",
    "df.loc[df.stemmed_words=='judentum','stemmed_words'] = 'Judentum'\n",
    "df.loc[df.stemmed_words=='katholik','stemmed_words'] = 'Katholizismus'\n",
    "df.loc[df.stemmed_words=='katholisch','stemmed_words'] = 'Katholizismus'\n",
    "df.loc[df.stemmed_words=='jehova','stemmed_words'] = 'Jehova'\n",
    "df.loc[df.stemmed_words=='protesta', 'stemmed_words'] = 'Protestantismus'\n",
    "df.loc[df.stemmed_words=='protestantisch', 'stemmed_words'] = 'Protestantismus'\n",
    "df.loc[df.stemmed_words=='pro', 'stemmed_words'] = 'Protestantismus'\n",
    "df.loc[df.stemmed_words=='muslim','stemmed_words'] = 'Islam'\n",
    "df.loc[df.stemmed_words=='muslimisch','stemmed_words'] = 'Islam'\n",
    "df.loc[df.stemmed_words=='islam','stemmed_words'] = 'Islam'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating backups\n",
    "#headers_list = ['suggestion_id', 'label', 'queryterm', 'date', 'client', 'lang', 'geolocation', 'url', 'proxy_ip', 'proxy_city', 'proxy_country', 'proxy_port', 'raw_data', 'cleaned', 'tokens_suggestions', 'tokensroot', 'Frage?', 'tokens_suggestions_cleaned_root', 'stemmed_words', 'synsets_ger', 'lexunits', 'hypernyms', 'lexunits_hypernyms', 'hyponyms', 'lexunits_hyponyms', 'lexunits_tensor', 'lexunits_tensor_sum', 'tokens_suggestions_tensor']\n",
    "df_backup = df\n",
    "df_backup2 = df\n",
    "df_backup3 = df\n",
    "df_backup4 = df\n",
    "df.to_csv('richtig.csv', sep=';', index=False, encoding='utf-8')\n",
    "df_from_csv = pd.read_csv(\"richtig.csv\", sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "source": [
    "rslt_df = df.loc[df['Frage?'] == 0]\n",
    "rslt_df1 = df.loc[df['Frage?'] == 1]\n",
    "print('keine Frage: '+str(len(rslt_df.index)))\n",
    "print('Frage: '+str(len(rslt_df1.index)))\n",
    "rslt_df1.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "!pip install jupyterlab\n",
     "\n",
     "\n",
     "import sys\n",
     "!{sys.executable} -m pip install -U germanetpy==0.2.2\n",
     "!pip install germanetpy==0.2.2\n",
     "from pathlib import Path\n",
     "from germanetpy import germanet\n",
     "\n",
     "\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}